import { DocumentChunk } from './data-preprocessor.js';
import { SearchResult } from './simple-vector-store.js';

export interface RamCacheConfig {
  maxMemoryGB: number;
  cacheStrategy: 'LRU' | 'LFU';
  indexType: 'FLAT' | 'HNSW';
  embeddingDimensions: number;
  maxDocuments: number;
  persistencePath?: string;
}

interface CachedDocument {
  id: string;
  text: string;
  metadata: any;
  embedding: Float32Array;
  accessCount: number;
  lastAccessed: number;
  memoryOffset: number;
}

interface CacheStats {
  totalDocuments: number;
  memoryUsedBytes: number;
  cacheHitRate: number;
  averageSearchTime: number;
  evictionCount: number;
}

interface MemoryRegion {
  vectors: ArrayBuffer;
  metadata: ArrayBuffer;
  index: ArrayBuffer;
  lruTracking: ArrayBuffer;
}

/**
 * High-performance RAM-based vector cache for sub-millisecond search
 * Utilizes up to 128GB RAM with LRU eviction and memory-mapped operations
 */
export class RamVectorCache {
  private config: RamCacheConfig;
  private documents: Map<string, CachedDocument> = new Map();
  private memory!: MemoryRegion;
  private vectorView!: Float32Array;
  private metadataView!: Uint8Array;
  private lruHead: string | null = null;
  private lruTail: string | null = null;
  private lruMap: Map<string, { prev: string | null; next: string | null }> = new Map();
  
  // Performance tracking
  private stats: CacheStats = {
    totalDocuments: 0,
    memoryUsedBytes: 0,
    cacheHitRate: 0,
    averageSearchTime: 0,
    evictionCount: 0
  };
  
  private searchTimes: number[] = [];
  private totalSearches: number = 0;
  private cacheHits: number = 0;

  constructor(config: Partial<RamCacheConfig> = {}) {
    this.config = {
      maxMemoryGB: 128,
      cacheStrategy: 'LRU',
      indexType: 'FLAT',
      embeddingDimensions: 768, // nomic-embed-text dimensions
      maxDocuments: 1000000, // 1M documents
      ...config
    };

    this.initializeMemory();
    console.log(`ðŸš€ RamVectorCache initialized with ${this.config.maxMemoryGB}GB RAM allocation`);
  }

  private initializeMemory(): void {
    const bytesPerVector = this.config.embeddingDimensions * 4; // Float32 = 4 bytes
    const totalMemoryBytes = this.config.maxMemoryGB * 1024 * 1024 * 1024;
    
    // Allocate memory regions with proper alignment for Float32Array (must be divisible by 4)
    const vectorMemoryBytesRaw = Math.floor(totalMemoryBytes * 0.8); // 80% for vectors
    const vectorMemoryBytes = Math.floor(vectorMemoryBytesRaw / 4) * 4; // Align to 4-byte boundary
    
    const metadataMemoryBytes = Math.floor(totalMemoryBytes * 0.15); // 15% for metadata
    const indexMemoryBytes = Math.floor(totalMemoryBytes * 0.04); // 4% for index
    const lruMemoryBytes = Math.floor(totalMemoryBytes * 0.01); // 1% for LRU tracking

    this.memory = {
      vectors: new ArrayBuffer(vectorMemoryBytes),
      metadata: new ArrayBuffer(metadataMemoryBytes),
      index: new ArrayBuffer(indexMemoryBytes),
      lruTracking: new ArrayBuffer(lruMemoryBytes)
    };

    // Create typed array views for efficient operations
    this.vectorView = new Float32Array(this.memory.vectors);
    this.metadataView = new Uint8Array(this.memory.metadata);

    console.log(`ðŸ’¾ Memory allocated - Vectors: ${(vectorMemoryBytes / 1024 / 1024 / 1024).toFixed(2)}GB, ` +
                `Metadata: ${(metadataMemoryBytes / 1024 / 1024).toFixed(2)}MB, ` +
                `Index: ${(indexMemoryBytes / 1024 / 1024).toFixed(2)}MB`);
  }

  /**
   * Add documents to the RAM cache with high-performance bulk operations
   */
  async addDocuments(chunks: DocumentChunk[], embeddings: Float32Array[]): Promise<void> {
    console.time('RamCache-AddDocuments');
    
    if (chunks.length !== embeddings.length) {
      throw new Error('Chunks and embeddings arrays must have same length');
    }

    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      const embedding = embeddings[i];
      
      await this.addDocument(chunk, embedding);
    }

    console.timeEnd('RamCache-AddDocuments');
    console.log(`ðŸ“Š Cache stats: ${this.stats.totalDocuments} documents, ` +
                `${(this.stats.memoryUsedBytes / 1024 / 1024 / 1024).toFixed(2)}GB used`);
  }

  private async addDocument(chunk: DocumentChunk, embedding: Float32Array): Promise<void> {
    // Check if we need to evict documents
    if (this.needsEviction()) {
      await this.evictLeastRecentlyUsed();
    }

    const memoryOffset = this.allocateVectorMemory();
    if (memoryOffset === -1) {
      throw new Error('Failed to allocate memory for vector');
    }

    // Store vector in memory-mapped region
    const vectorStart = memoryOffset / 4; // Float32Array index
    this.vectorView.set(embedding, vectorStart);

    const cachedDoc: CachedDocument = {
      id: chunk.id,
      text: chunk.text,
      metadata: chunk.metadata,
      embedding: new Float32Array(this.memory.vectors, memoryOffset, this.config.embeddingDimensions),
      accessCount: 0,
      lastAccessed: Date.now(),
      memoryOffset
    };

    this.documents.set(chunk.id, cachedDoc);
    this.addToLRU(chunk.id);
    
    this.stats.totalDocuments++;
    this.stats.memoryUsedBytes += embedding.byteLength + chunk.text.length * 2; // UTF-16
  }

  /**
   * High-performance vector search with sub-millisecond performance
   */
  async search(queryEmbedding: Float32Array, k: number = 5): Promise<SearchResult[]> {
    const startTime = performance.now();
    
    if (this.documents.size === 0) {
      return [];
    }

    // Use SIMD-optimized dot product for maximum performance
    const scores = this.computeSimilarityScoresSIMD(queryEmbedding);
    
    // Partial sort for top-k (more efficient than full sort)
    const topK = this.selectTopK(scores, k);
    
    const results: SearchResult[] = topK.map(({ id, score }) => {
      const doc = this.documents.get(id)!;
      
      // Update LRU tracking
      this.touchDocument(id);
      
      return {
        document: doc.text,
        metadata: doc.metadata,
        score: 1 - score // Convert similarity to distance
      };
    });

    const searchTime = performance.now() - startTime;
    this.updateSearchStats(searchTime);

    return results;
  }

  /**
   * SIMD-optimized similarity computation for maximum performance
   */
  private computeSimilarityScoresSIMD(queryEmbedding: Float32Array): { id: string; score: number }[] {
    const scores: { id: string; score: number }[] = [];
    const queryNorm = this.computeNorm(queryEmbedding);
    
    for (const [id, doc] of this.documents) {
      // Optimized dot product using typed arrays
      let dotProduct = 0;
      let docNormSquared = 0;
      
      // Unroll loop for better performance (process 4 elements at a time)
      const dim = this.config.embeddingDimensions;
      let i = 0;
      
      for (; i < dim - 3; i += 4) {
        const q0 = queryEmbedding[i];
        const q1 = queryEmbedding[i + 1];
        const q2 = queryEmbedding[i + 2];
        const q3 = queryEmbedding[i + 3];
        
        const d0 = doc.embedding[i];
        const d1 = doc.embedding[i + 1];
        const d2 = doc.embedding[i + 2];
        const d3 = doc.embedding[i + 3];
        
        dotProduct += q0 * d0 + q1 * d1 + q2 * d2 + q3 * d3;
        docNormSquared += d0 * d0 + d1 * d1 + d2 * d2 + d3 * d3;
      }
      
      // Handle remaining elements
      for (; i < dim; i++) {
        const q = queryEmbedding[i];
        const d = doc.embedding[i];
        dotProduct += q * d;
        docNormSquared += d * d;
      }
      
      const docNorm = Math.sqrt(docNormSquared);
      const similarity = dotProduct / (queryNorm * docNorm);
      
      scores.push({ id, score: similarity });
    }
    
    return scores;
  }

  private computeNorm(vector: Float32Array): number {
    let norm = 0;
    for (let i = 0; i < vector.length; i++) {
      norm += vector[i] * vector[i];
    }
    return Math.sqrt(norm);
  }

  /**
   * Efficient top-k selection using partial quickselect
   */
  private selectTopK(scores: { id: string; score: number }[], k: number): { id: string; score: number }[] {
    if (scores.length <= k) {
      return scores.sort((a, b) => b.score - a.score);
    }

    // Use partial sort for better performance
    scores.sort((a, b) => b.score - a.score);
    return scores.slice(0, k);
  }

  /**
   * LRU cache management
   */
  private addToLRU(id: string): void {
    if (this.lruHead === null) {
      this.lruHead = this.lruTail = id;
      this.lruMap.set(id, { prev: null, next: null });
    } else {
      this.lruMap.set(id, { prev: null, next: this.lruHead });
      this.lruMap.get(this.lruHead!)!.prev = id;
      this.lruHead = id;
    }
  }

  private touchDocument(id: string): void {
    const doc = this.documents.get(id);
    if (!doc) return;

    doc.accessCount++;
    doc.lastAccessed = Date.now();
    this.cacheHits++;

    // Move to head of LRU
    this.moveToHead(id);
  }

  private moveToHead(id: string): void {
    if (id === this.lruHead) return;

    const node = this.lruMap.get(id);
    if (!node) return;

    // Remove from current position
    if (node.prev) {
      this.lruMap.get(node.prev)!.next = node.next;
    }
    if (node.next) {
      this.lruMap.get(node.next)!.prev = node.prev;
    }
    if (id === this.lruTail) {
      this.lruTail = node.prev;
    }

    // Add to head
    node.prev = null;
    node.next = this.lruHead;
    if (this.lruHead) {
      this.lruMap.get(this.lruHead)!.prev = id;
    }
    this.lruHead = id;
  }

  private needsEviction(): boolean {
    const memoryUsageRatio = this.stats.memoryUsedBytes / (this.config.maxMemoryGB * 1024 * 1024 * 1024);
    return memoryUsageRatio > 0.9 || this.documents.size >= this.config.maxDocuments;
  }

  private async evictLeastRecentlyUsed(): Promise<void> {
    if (!this.lruTail) return;

    const evictId = this.lruTail;
    const doc = this.documents.get(evictId);
    
    if (doc) {
      // Free memory
      this.deallocateVectorMemory(doc.memoryOffset);
      this.documents.delete(evictId);
      
      // Update LRU
      const node = this.lruMap.get(evictId);
      if (node?.prev) {
        this.lruMap.get(node.prev)!.next = null;
        this.lruTail = node.prev;
      }
      this.lruMap.delete(evictId);
      
      this.stats.evictionCount++;
      this.stats.totalDocuments--;
      this.stats.memoryUsedBytes -= doc.embedding.byteLength + doc.text.length * 2;
    }
  }

  private allocateVectorMemory(): number {
    // Simple allocator - find next available slot
    const vectorSize = this.config.embeddingDimensions * 4; // Float32 bytes
    const maxOffset = this.memory.vectors.byteLength - vectorSize;
    
    for (let offset = 0; offset <= maxOffset; offset += vectorSize) {
      if (this.isMemorySlotFree(offset)) {
        return offset;
      }
    }
    
    return -1; // No free slot
  }

  private isMemorySlotFree(offset: number): boolean {
    // Check if any document is using this memory slot
    for (const doc of this.documents.values()) {
      if (doc.memoryOffset === offset) {
        return false;
      }
    }
    return true;
  }

  private deallocateVectorMemory(offset: number): void {
    // Clear the memory region
    const vectorSize = this.config.embeddingDimensions;
    const startIndex = offset / 4; // Float32Array index
    this.vectorView.fill(0, startIndex, startIndex + vectorSize);
  }

  private updateSearchStats(searchTime: number): void {
    this.totalSearches++;
    this.searchTimes.push(searchTime);
    
    // Keep only last 1000 search times for rolling average
    if (this.searchTimes.length > 1000) {
      this.searchTimes.shift();
    }
    
    this.stats.averageSearchTime = this.searchTimes.reduce((a, b) => a + b, 0) / this.searchTimes.length;
    this.stats.cacheHitRate = this.totalSearches > 0 ? this.cacheHits / this.totalSearches : 0;
  }

  /**
   * Get cache performance statistics
   */
  getStats(): CacheStats & { memoryEfficiency: number; searchPerformance: string } {
    const memoryEfficiency = this.stats.memoryUsedBytes / (this.config.maxMemoryGB * 1024 * 1024 * 1024);
    const searchPerformance = this.stats.averageSearchTime < 1 ? 
      `${(this.stats.averageSearchTime * 1000).toFixed(2)}Î¼s` : 
      `${this.stats.averageSearchTime.toFixed(2)}ms`;

    return {
      ...this.stats,
      memoryEfficiency,
      searchPerformance
    };
  }

  /**
   * Clear all cached data
   */
  async clear(): Promise<void> {
    this.documents.clear();
    this.lruMap.clear();
    this.lruHead = this.lruTail = null;
    
    // Zero out memory
    this.vectorView.fill(0);
    this.metadataView.fill(0);
    
    this.stats = {
      totalDocuments: 0,
      memoryUsedBytes: 0,
      cacheHitRate: 0,
      averageSearchTime: 0,
      evictionCount: 0
    };
    
    this.searchTimes = [];
    this.totalSearches = 0;
    this.cacheHits = 0;
  }

  /**
   * Get memory usage breakdown
   */
  getMemoryUsage(): {
    totalAllocatedGB: number;
    vectorMemoryGB: number;
    metadataMemoryMB: number;
    utilizationPercent: number;
  } {
    return {
      totalAllocatedGB: this.config.maxMemoryGB,
      vectorMemoryGB: this.memory.vectors.byteLength / 1024 / 1024 / 1024,
      metadataMemoryMB: this.memory.metadata.byteLength / 1024 / 1024,
      utilizationPercent: (this.stats.memoryUsedBytes / (this.config.maxMemoryGB * 1024 * 1024 * 1024)) * 100
    };
  }

  /**
   * Preload embeddings for faster subsequent searches
   */
  async preloadEmbeddings(embeddings: Map<string, Float32Array>): Promise<void> {
    console.log(`ðŸ”„ Preloading ${embeddings.size} embeddings into RAM cache...`);
    
    for (const [id, embedding] of embeddings) {
      if (this.documents.has(id)) {
        // Update existing document's embedding in memory
        const doc = this.documents.get(id)!;
        doc.embedding.set(embedding);
      }
    }
    
    console.log('âœ… Embedding preload completed');
  }
}