services:
  canvasbot:
    build: .
    container_name: canvasbot-api
    ports:
      - "0.0.0.0:3001:3001"  # Network accessible by default
    environment:
      - NODE_ENV=production
      - PORT=3001
      - CANVAS_BASE_URL=${CANVAS_BASE_URL}
      - CANVAS_ACCESS_TOKEN=${CANVAS_ACCESS_TOKEN}
      - STUDENT_ID=${STUDENT_ID}
      - VLLM_BASE_URL=http://vllm:8000
      - VLLM_MODEL=${VLLM_MODEL:-Qwen/Qwen2.5-32B-Instruct-AWQ}
      - OLLAMA_BASE_URL=http://ollama:11434
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
    volumes:
      - ./cache:/app/cache
      - ./logs:/app/logs
      - ./.env:/app/.env:ro
    restart: unless-stopped
    depends_on:
      - vllm
      - ollama
    networks:
      - canvasbot-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # vLLM High-Performance Inference Engine
  vllm:
    image: vllm/vllm-openai:latest
    container_name: canvasbot-vllm
    ports:
      - "0.0.0.0:8000:8000"
    command: ["--model", "${VLLM_MODEL:-Qwen/Qwen2.5-32B-Instruct-AWQ}", "--trust-remote-code", "--tensor-parallel-size", "${VLLM_GPU_COUNT:-1}", "--max-model-len", "${VLLM_MAX_LENGTH:-10176}"]
    environment:
      - MODEL_NAME=${VLLM_MODEL:-Qwen/Qwen2.5-32B-Instruct-AWQ}
      - TENSOR_PARALLEL_SIZE=${VLLM_GPU_COUNT:-1}
      - MAX_MODEL_LEN=${VLLM_MAX_LENGTH:-10176}
      - TRUST_REMOTE_CODE=true
      # NCCL debugging and optimization for multi-GPU tensor parallelism
      - NCCL_DEBUG=INFO
      - NCCL_DEBUG_SUBSYS=ALL
      - NCCL_SOCKET_IFNAME=^docker0,lo
      - NCCL_IB_DISABLE=1
      - NCCL_P2P_DISABLE=0
      - NCCL_SHM_DISABLE=0
      - CUDA_VISIBLE_DEVICES=0,1
    volumes:
      - vllm_cache:/root/.cache
      - ./models:/models
    # Allocate sufficient shared memory for NCCL inter-GPU communication
    shm_size: '2g'
    restart: unless-stopped
    networks:
      - canvasbot-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Requires GPUs 0,1 for tensor parallelism
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']
              capabilities: [gpu]

  # Ollama for Embeddings Only
  ollama:
    image: ollama/ollama:latest
    container_name: canvasbot-embeddings
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    networks:
      - canvasbot-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]

  # Open WebUI Interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:0.6.23
    container_name: canvasbot-webui
    ports:
      - "0.0.0.0:8081:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=http://canvasbot:3001/v1
      - OPENAI_API_KEY=canvasbot-key
      # Authentication Configuration
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-canvasbot-secret-key}
      # Development Settings
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-user}
    volumes:
      - open_webui_data:/app/backend/data
    restart: unless-stopped
    depends_on:
      - vllm
      - ollama
      - canvasbot
    networks:
      - canvasbot-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama_data:
  open_webui_data:
  vllm_cache:

networks:
  canvasbot-network:
    driver: bridge